# <p align=center>Video Depth Estimation Rankings<br />and Stereo Video Conversion Rankings</p>

**Researchers**, if you have found your way here, please consider developing a new Stereo Video Conversion model with a resolution of 1080p and 24fps based on the **Wan2.5** backbone, which should soon be available to everyone. 

Recently, three research papers describing new Stereo Video Conversion models appeared almost simultaneously, including two based on the Wan2.1-T2V-1.3B backbone. Unfortunately, the low resolution of 480p and 16fps does not provide full immersion, despite the fact that the Wan backbone offers significantly higher quality than the Stable Video Diffusion (SVD) backbone - [see the comparison](https://github.com/Wan-Video/Wan2.2/blob/main/assets/vae.png). However, I must emphasise that I am very pleased that the new models are raising the bar in terms of quality once again. So pleased, in fact, that I want more...

| backbone | open source | resolution | fps | video length | Stereo Video Conversion models |
|:---:|:---:|:---:|:---:|:---:|:---:|
| [Wan2.1-T2V-1.3B](https://github.com/Wan-Video/Wan2.1) | Yes | 832√ó480 | 16 | 5s | [**HairGuard**](https://arxiv.org/abs/2601.03362)<br />[**StereoPilot**](https://hit-perfect.github.io/StereoPilot/)<br />[**StereoWorld**](https://ke-xing.github.io/StereoWorld/) |
| [Wan2.2-TI2V-5B](https://github.com/Wan-Video/Wan2.2) | Yes | 1280√ó704 | 24 | 5s | - |
| [Wan2.2-T2V-A14B](https://github.com/Wan-Video/Wan2.2) | Yes | 1280√ó720 | 16 | 5s | - |
| Wan2.5 | Yes? | 1080p | 24 | 10s | - |
| Wan2.6 | No | 1080p | 24 | 15s | - |

## Awesome Stereo Video Conversion
The following list includes **all Stereo Video Conversion methods from the last 10 months, from 1 April 2025 to 1 February 2026**.
This list was created because there is a significant problem with public access to the latest Stereo Video Conversion models, which makes it difficult for researchers to compare their work with the current state of the art and to use the same test set. Consequently, this also makes it difficult to present a single ranking that showcases all of the best models.
| Method | Backbone | Submitted&nbsp;on<br />(arXiv) | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Official<br />&nbsp;&nbsp;repository&nbsp;&nbsp; | Code<br />(website) |
|:---:|:---:|:---:|:---:|:---:|:---:|
| HairGuard | VACE (based on Wan2.1-1.3B) | 6 Jan 2026 | [![arXiv](https://img.shields.io/badge/2026-arXiv-b31b1b)](https://arxiv.org/abs/2601.03362) | - | - |
| StereoPilot | Wan2.1-T2V-1.3B | 18 Dec 2025 | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2512.16915) | [![GitHub Stars](https://img.shields.io/github/stars/KlingTeam/StereoPilot)](https://github.com/KlingTeam/StereoPilot) | [![GitHub Stars](https://img.shields.io/github/stars/hit-perfect/StereoPilot)](https://github.com/hit-perfect/StereoPilot) |
| Elastic3D | SVD | 16 Dec 2025 | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2512.14236) | - | [![GitHub Stars](https://img.shields.io/github/stars/elastic3d/elastic3d.github.io)](https://github.com/elastic3d/elastic3d.github.io) |
| StereoWorld | Wan2.1-T2V-1.3B | 10 Dec 2025 | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2512.09363) | - | [![GitHub Stars](https://img.shields.io/github/stars/ke-xing/StereoWorld)](https://github.com/ke-xing/StereoWorld) |
| Restereo | StereoCrafter (based on SVD) | 6 Jun 2025 | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2506.06023) | - | - |
| M2SVid | SVD | 22 May 2025 | [![3DV](https://img.shields.io/badge/2026-3DV-8fcaff)](https://arxiv.org/abs/2505.16565) | - | [![GitHub Stars](https://img.shields.io/github/stars/m2svid/m2svid.github.io)](https://github.com/m2svid/m2svid.github.io) |
| Eye2Eye | Lumiere | 30 Apr 2025 | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2505.00135) | - | [![GitHub Stars](https://img.shields.io/github/stars/video-Eye2Eye/video-eye2eye.github.io)](https://github.com/video-Eye2Eye/video-eye2eye.github.io) |

--------------

## Awesome Synthetic RGB-D Image Datasets for Training HD Video Depth Estimation Models
Although video depth estimation models should be trained mainly on synthetic RGB-D video datasets I decided to add two synthetic RGB-D image datasets because of their unique features. 
|  | Dataset | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Resolution | Unique features |
|:---:|:---:|:---:|:---:|:---:|
| 1 | **[SynthHuman](https://microsoft.github.io/DAViD/)<br />üìå&nbsp;Human&nbsp;faces&nbsp;üòç** | [![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Saleh_DAViD_Data-efficient_and_Accurate_Vision_Models_from_Synthetic_Data_ICCV_2025_paper.html) | **384√ó512** | The dataset contains 98040 samples feature the face, 99976 sample feature the full body and 99992 samples feature the upper body. DAViD trained on this dataset alone achieved better depth estimation results than Depth Anything V2 Large, Depth Pro and even Sapiens-2B on the Goliath-Face test set. See the results in [Table 1](https://openaccess.thecvf.com/content/ICCV2025/html/Saleh_DAViD_Data-efficient_and_Accurate_Vision_Models_from_Synthetic_Data_ICCV_2025_paper.html). |
| 2 | **[MegaSynth](https://hwjiang1510.github.io/MegaSynth/)** | [![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_MegaSynth_Scaling_Up_3D_Scene_Reconstruction_with_Synthesized_Data_CVPR_2025_paper.html) | **512√ó512** | Huge size: 700K scenes and the incredible improvement in depth estimation results of the fine-tuned Depth Anything V2 ViT-B model on MegaSynth and evaluated on Hypersim. See the results in [Table 6](https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_MegaSynth_Scaling_Up_3D_Scene_Reconstruction_with_Synthesized_Data_CVPR_2025_paper.html). |

## Awesome Synthetic RGB-D Video Datasets for Training and Testing HD Video Depth Estimation Models
The following list contains only synthetic RGB-D datasets in which at least some of the images can be composited into a video sequence of at least 32 frames. The minimum number of frames was chosen on the basis of the ablation studies shown in [Table 5](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html) by the Video Depth Anything researchers.  

Most datasets contain ready-to-use video sequences of appropriately numbered images in individual folders, but in the case of the [PLT-D3](https://doi.org/10.7910/DVN/36SQKM) dataset, images from at least two folders have to be combined to make a longer video sequence and in the case of the [ClaraVid](https://rdbch.github.io/claravid/) dataset, images have to be arranged in the correct order to make a 32-frame video sequence, for example in the order given in [Appendix 4: Notes for "Awesome Synthetic RGB-D Video Datasets for Training and Testing HD Video Depth Estimation Models"](#appendix-4-notes-for-awesome-synthetic-rgb-d-video-datasets-for-training-and-testing-hd-video-depth-estimation-models).

Researchers, if you are going to use the following list to select datasets to train your models check their quality very carefully and choose the best ones. I have only visually checked a few of them and have marked on the list 2 datasets to check particularly carefully and 2 datasets that in my opinion are not suitable for training video depth estimation models. I have given the reasons for such markings in the same [Appendix 4](#appendix-4-notes-for-awesome-synthetic-rgb-d-video-datasets-for-training-and-testing-hd-video-depth-estimation-models).

In selecting the best datasets, comparisons of their quality can be very helpful, such as in [Table 9](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html), [Table 6](https://arxiv.org/abs/2509.12201), another [Table 6](https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_MegaSynth_Scaling_Up_3D_Scene_Reconstruction_with_Synthesized_Data_CVPR_2025_paper.html) for depth estimation models and [TABLE V](https://arxiv.org/abs/2411.14053) plus [TABLE IV](https://arxiv.org/abs/2411.14053) for stereo matching models, although a similar technique can also be used for depth estimation models.
|  | Dataset | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Resolution | [D<br />A<br />3<br />T](https://arxiv.org/abs/2511.10647 "Depth Anything 3 Teacher") | [G<br />C](https://openaccess.thecvf.com/content/ICCV2025/html/Xu_GeometryCrafter_Consistent_Geometry_Estimation_for_Open-world_Videos_with_Diffusion_Priors_ICCV_2025_paper.html "GeometryCrafter") | [M<br />o<br />2](https://arxiv.org/abs/2507.02546 "MoGe-2") | [D<br />P](https://openreview.net/forum?id=aueXfY0Clv "Depth Pro") | [S<br />T<br />2](https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_SpatialTrackerV2_Advancing_3D_Point_Tracking_with_Explicit_Camera_Motion_ICCV_2025_paper.html "SpatialTrackerV2") | [U<br />D<br />2](https://arxiv.org/abs/2502.20110 "UniDepthV2") | [V<br />D<br />A](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html "Video Depth Anything") | [D<br /><sup>2</sup><br />U](https://arxiv.org/abs/2504.06264 "D^2USt3R") | [P<br />O<br />M](https://arxiv.org/abs/2504.05692 "POMATO") | [R<br />D](https://openaccess.thecvf.com/content/CVPR2025/html/Ke_Video_Depth_without_Video_Models_CVPR_2025_paper.html "RollingDepth") | [B<br />o<br />T](https://arxiv.org/abs/2504.14516 "Back on Track") |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 1 | **[BEDLAM2.0](https://bedlam2.is.tuebingen.mpg.de/)<br />üìå&nbsp;Human&nbsp;poses&nbsp;üòç** | [![NeurIPS](https://img.shields.io/badge/2025-NeurIPS-68448a)](https://openreview.net/forum?id=ii9kVwf95a) | **1280√ó720** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 2 | **[OmniWorld-Game](https://yangzhou24.github.io/OmniWorld/)<br />üìå&nbsp;18,515K&nbsp;frames&nbsp;üòç** | [![ICLR](https://img.shields.io/badge/2026-ICLR-d5df32)](https://openreview.net/forum?id=1y1YFKb9pp) | **1280√ó720** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 3 | **[C3I-SynFace](https://github.com/shubhajitbasak/blenderDataGeneration)<br />üìå&nbsp;Human&nbsp;faces&nbsp;üòç** | [![DIB](https://img.shields.io/badge/2023-DIB-e01e26)](https://www.sciencedirect.com/science/article/pii/S2352340923002068) | **640√ó480** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 4 | **[ClaraVid](https://rdbch.github.io/claravid/)** | [![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Beche_ClaraVid_A_Holistic_Scene_Reconstruction_Benchmark_From_Aerial_Perspective_With_ICCV_2025_paper.html) | **4032x3024** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 5 | **[Spring](https://spring-benchmark.org/)** | [![CVPR](https://img.shields.io/badge/2023-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2023/html/Mehl_Spring_A_High-Resolution_High-Detail_Dataset_and_Benchmark_for_Scene_Flow_CVPR_2023_paper.html) | **1920√ó1080** | **T** | **T** | **E** | **E** | **T** | **-** | **-** | **T** | **-** | **-** | **-** |
| 6 | **[HorizonGS](https://city-super.github.io/horizon-gs/)** | [![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Horizon-GS_Unified_3D_Gaussian_Splatting_for_Large-Scale_Aerial-to-Ground_Scenes_CVPR_2025_paper.html) | **1920√ó1080** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 7 | **[PLT-D3](https://doi.org/10.7910/DVN/36SQKM)** | [![HD](https://img.shields.io/badge/2024-HD-c55b28)](https://arxiv.org/abs/2406.07667) | **1920√ó1080** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 8 | **[MVS-Synth](https://phuang17.github.io/DeepMVS/mvs-synth.html)** | [![CVPR](https://img.shields.io/badge/2018-CVPR-1e407f)](https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper.html) | **1920√ó1080** | **T** | **T** | **T** | **T** | **T** | **-** | **-** | **-** | **-** | **-** | **-** |
| 9 | **[SYNTHIA-SF](https://synthia-dataset.net/)** | [![BMVC](https://img.shields.io/badge/2017-BMVC-555676)](https://bmva-archive.org.uk/bmvc/2017/papers/paper087/index.html) | **1920√ó1080** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 10 | **[SynDrone](https://github.com/LTTM/Syndrone)<br />Check&nbsp;before&nbsp;use!** | [![ICCVW](https://img.shields.io/badge/2023-ICCVW-fcb900)](https://openaccess.thecvf.com/content/ICCV2023W/WiCV/html/Rizzoli_SynDrone_-_Multi-Modal_UAV_Dataset_for_Urban_Scenarios_ICCVW_2023_paper.html) | **1920√ó1080** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 11 | **[Mid-Air](https://midair.ulg.ac.be/)** | [![CVPRW](https://img.shields.io/badge/2019-CVPRW-1e407f)](https://openaccess.thecvf.com/content_CVPRW_2019/html/UAVision/Fonder_Mid-Air_A_Multi-Modal_Dataset_for_Extremely_Low_Altitude_Drone_Flights_CVPRW_2019_paper.html) | **1024√ó1024** | **-** | **T** | **T** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 12 | **[MatrixCity](https://city-super.github.io/matrixcity/)** | [![ICCV](https://img.shields.io/badge/2023-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2023/html/Li_MatrixCity_A_Large-scale_City_Dataset_for_City-scale_Neural_Rendering_and_ICCV_2023_paper.html) | **1000√ó1000** | **T** | **T** | **T** | **-** | **-** | **T** | **-** | **-** | **-** | **-** | **-** |
| 13 | **[StereoCarla](https://xiandaguo.net/StereoCarla/)** | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2509.12683) | **1600√ó900** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 14 | **[LightwheelOcc](https://github.com/OpenDriveLab/LightwheelOcc)** | - | **1600√ó900** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 15 | **[SAIL-VOS 3D](https://sailvos.web.illinois.edu/_site/_site/index.html)** | [![CVPR](https://img.shields.io/badge/2021-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_SAIL-VOS_3D_A_Synthetic_Dataset_and_Baselines_for_Object_Detection_CVPR_2021_paper.html) | **1280√ó800** | **-** | **-** | **-** | **T** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 16 | **[SHIFT](https://github.com/SysCV/shift-dev)** | [![CVPR](https://img.shields.io/badge/2022-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2022/html/Sun_SHIFT_A_Synthetic_Driving_Dataset_for_Continuous_Multi-Task_Domain_Adaptation_CVPR_2022_paper.html) | **1280√ó800** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 17 | **[SYNTHIA-Seqs](https://synthia-dataset.net/)<br />üö´&nbsp;Do&nbsp;not&nbsp;use!&nbsp;üö´** | [![CVPR](https://img.shields.io/badge/2016-CVPR-1e407f)](https://openaccess.thecvf.com/content_cvpr_2016/html/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.html) | **1280√ó760** | **-** | **T** | **T** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 18 | **[BEDLAM](https://bedlam.is.tue.mpg.de/)** | [![CVPR](https://img.shields.io/badge/2023-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2023/html/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.html) | **1280√ó720** | **-** | **-** | **-** | **T** | **T** | **T** | **-** | **-** | **-** | **-** | **-** |
| 19 | **[Dynamic Replica](https://dynamic-stereo.github.io/)** | [![CVPR](https://img.shields.io/badge/2023-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2023/html/Karaev_DynamicStereo_Consistent_Dynamic_Depth_From_Stereo_Videos_CVPR_2023_paper.html) | **1280√ó720** | **-** | **T** | **-** | **T** | **T** | **T** | **-** | **-** | **T** | **-** | **-** |
| 20 | **[Infinigen SV](https://tomtomtommi.github.io/BiDAVideo/)** | [![arXiv](https://img.shields.io/badge/2024-arXiv-b31b1b)](https://arxiv.org/abs/2409.20283) | **1280√ó720** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 21 | **[Infinigen](https://infinigen.org/)** | [![CVPR](https://img.shields.io/badge/2023-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2023/html/Raistrick_Infinite_Photorealistic_Worlds_Using_Procedural_Generation_CVPR_2023_paper.html) | **1280√ó720** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 22 | **[DigiDogs](https://cvssp.org/data/DigiDogs/)<br />üö´&nbsp;Do&nbsp;not&nbsp;use!&nbsp;üö´** | [![WACVW](https://img.shields.io/badge/2024-WACVW-2e0094)](https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Shooter_DigiDogs_Single-View_3D_Pose_Estimation_of_Dogs_Using_Synthetic_Training_WACVW_2024_paper.html) | **1280√ó720** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 23 | **[Aria Synthetic Environments](https://www.projectaria.com/datasets/ase/)<br />Check&nbsp;before&nbsp;use!** | - | **704√ó704** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 24 | **[TartanGround](https://tartanair.org/tartanground/)** | [![IROS](https://img.shields.io/badge/2025-IROS-007475)](https://arxiv.org/abs/2505.10696) | **640√ó640** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 25 | **[TartanAir V2](https://tartanair.org/)** | - | **640√ó640** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 26 | **[BlinkVision](https://www.blinkvision.org/)** | [![ECCV](https://img.shields.io/badge/2024-ECCV-67cd84)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8381_ECCV_2024_paper.php) | **960√ó540** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **T** | **-** | **-** | **-** |
| 27 | **[PointOdyssey](https://pointodyssey.com/)** | [![ICCV](https://img.shields.io/badge/2023-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_PointOdyssey_A_Large-Scale_Synthetic_Dataset_for_Long-Term_Point_Tracking_ICCV_2023_paper.html) | **960√ó540** | **T** | **-** | **-** | **-** | **T** | **T** | **T** | **T** | **T** | **E** | **-** |
| 28 | **[DyDToF](https://zhsun0357.github.io/consistent_dtof_video/)** | [![CVPR](https://img.shields.io/badge/2023-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Consistent_Direct_Time-of-Flight_Video_Depth_Super-Resolution_CVPR_2023_paper.html) | **960√ó540** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **E** | **-** |
| 29 | **[IRS](https://github.com/HKBU-HPML/IRS)** | [![ICME](https://img.shields.io/badge/2021-ICME-4afd01)](https://ieeexplore.ieee.org/document/9428423) | **960√ó540** | **T** | **T** | **T** | **T** | **-** | **-** | **T** | **-** | **-** | **-** | **-** |
| 30 | **[Scene Flow](https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html)** | [![CVPR](https://img.shields.io/badge/2016-CVPR-1e407f)](https://openaccess.thecvf.com/content_cvpr_2016/html/Mayer_A_Large_Dataset_CVPR_2016_paper.html) | **960√ó540** | **-** | **E** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 31 | **[THUD++](https://jackyzengl.github.io/THUD-plus-plus.github.io/)** | [![arXiv](https://img.shields.io/badge/2024-arXiv-b31b1b)](https://arxiv.org/abs/2412.08096) | **730√ó530** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 32 | **[TAU Agent](https://www.cs.toronto.edu/~harel/TAUAgent/home.html)** | [![TCI](https://img.shields.io/badge/2018-TCI-baffc9)](https://ieeexplore.ieee.org/document/8389203) | **1024√ó512** | **T** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 33 | **[TransPhy3D](https://daniellli.github.io/projects/DKT/)** | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2512.23705) | **512√ó512** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 34 | **[3D Ken Burns](https://github.com/sniklaus/3d-ken-burns)** | [![TOG](https://img.shields.io/badge/2019-TOG-cb2a3f)](https://dl.acm.org/doi/10.1145/3355089.3356528) | **512√ó512** | **T** | **T** | **T** | **T** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 35 | **[SynPhoRest](https://doi.org/10.5281/zenodo.6369445)** | - | **848√ó480** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 36 | **[TartanAir](https://theairlab.org/tartanair-dataset/)** | [![IROS](https://img.shields.io/badge/2020-IROS-007475)](https://ieeexplore.ieee.org/document/9341801) | **640√ó480** | **T** | **T** | **T** | **T** | **T** | **T** | **T** | **T** | **T** | **T** | **-** |
| 37 | **[ParallelDomain-4D](https://gcd.cs.columbia.edu/)** | [![ECCV](https://img.shields.io/badge/2024-ECCV-67cd84)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3510_ECCV_2024_paper.php) | **640√ó480** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **T** | **-** | **-** |
| 38 | **[EDEN](https://lhoangan.github.io/eden/)** | [![WACV](https://img.shields.io/badge/2021-WACV-2e0094)](https://openaccess.thecvf.com/content/WACV2021/html/Le_EDEN_Multimodal_Synthetic_Dataset_of_Enclosed_GarDEN_Scenes_WACV_2021_paper.html) | **640√ó480** | **T** | **-** | **T** | **T** | **-** | **T** | **-** | **-** | **-** | **-** | **-** |
| 39 | **[GTA-SfM](https://github.com/HKUST-Aerial-Robotics/Flow-Motion-Depth)** | [![RAL](https://img.shields.io/badge/2020-RAL-99002a)](https://ieeexplore.ieee.org/document/9006823) | **640√ó480** | **T** | **T** | **T** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 40 | **[InteriorNet](https://interiornet.org/)** | [![BMVC](https://img.shields.io/badge/2018-BMVC-555676)](https://arxiv.org/abs/1809.00716) | **640√ó480** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 41 | **[SYNTHIA-AL](https://synthia-dataset.net/)** | [![ICCVW](https://img.shields.io/badge/2019-ICCVW-fcb900)](https://openaccess.thecvf.com/content_ICCVW_2019/html/CVRSUAD/Bengar_Temporal_Coherence_for_Active_Learning_in_Videos_ICCVW_2019_paper.html) | **640√ó480** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** |
| 42 | **[MPI Sintel](http://sintel.is.tue.mpg.de/)** | [![ECCV](https://img.shields.io/badge/2012-ECCV-67cd84)](https://link.springer.com/chapter/10.1007/978-3-642-33868-7_17) | **1024√ó436** | **E** | **E** | **E** | **E** | **E** | **E** | **E** | **E** | **E** | **-** | **E** |
| 43 | **[Virtual KITTI 2](https://europe.naverlabs.com/proxy-virtual-worlds-vkitti-2/)** | [![arXiv](https://img.shields.io/badge/2020-arXiv-b31b1b)](https://arxiv.org/abs/2001.10773) | **1242√ó375** | **T** | **T** | **-** | **T** | **T** | **-** | **T** | **-** | **-** | **-** | **-** |
| 44 | **[TartanAir Shibuya](https://github.com/haleqiu/tartanair-shibuya)** | [![ICRA](https://img.shields.io/badge/2022-ICRA-73afd7)](https://ieeexplore.ieee.org/document/9811667) | **640√ó360** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **-** | **E** |
|  | **Total: T (training)** |  |  | **11** | **11** | **9** | **9** | **7** | **6** | **4** | **4** | **4** | **1** | **0** |
|  | **Total: E (testing)** |  |  | **1** | **2** | **2** | **2** | **1** | **1** | **1** | **1** | **1** | **2** | **2** |

--------------------

## <p align=center>List of Rankings</p>

### Stereo Video Conversion Rankings
1. [**Stereo4D (400 video clips with 16 frames each at 5 fps): LPIPS<=0.242**](#stereo4d-400-video-clips-with-16-frames-each-at-5-fps-lpips0242)
1. [**StereoWorld-11M (1000 video clips with 81 frames each at 12 fps): LPIPS<=0.1869**](#stereoworld-11m-1000-video-clips-with-81-frames-each-at-12-fps-lpips01869)
### Video Depth Estimation Rankings
1. [**Infinigen: OPW<=0.054**](#infinigen-opw0054)
1. [**ScanNet (170 frames): TAE<=2.2**](#scannet-170-frames-tae22)
1. [**Bonn RGB-D Dynamic (5 video clips with 110 frames each): Œ¥<sub>1</sub>>=0.979**](#bonn-rgb-d-dynamic-5-video-clips-with-110-frames-each-Œ¥10979)
1. [**Bonn RGB-D Dynamic (5 video clips with 110 frames each): AbsRel<=0.052**](#bonn-rgb-d-dynamic-5-video-clips-with-110-frames-each-absrel0052)
### Single Image Depth Estimation Rankings
1. [**DIODE: Œ¥<sub>1</sub>>=0.953**](#diode-Œ¥10953)
1. [**NYU-Depth V2: Œ¥<sub>1</sub>>=0.983**](#nyu-depth-v2-Œ¥10983)
1. [**NYU-Depth V2: AbsRel<=0.0421**](#nyu-depth-v2-absrel00421)
### Appendices
- **Appendix 1: Selection of rankings for this repository** (to do)
- **Appendix 2: Selection of metrics for the rankings** (to do)
- **Appendix 3: Rules for qualifying models for the rankings** (to do)
- [**Appendix 4: Notes for "Awesome Synthetic RGB-D Video Datasets for Training and Testing HD Video Depth Estimation Models"**](#appendix-4-notes-for-awesome-synthetic-rgb-d-video-datasets-for-training-and-testing-hd-video-depth-estimation-models)
- [**Appendix 5: List of all research papers from the above rankings**](#appendix-5-list-of-all-research-papers-from-the-above-rankings)

--------------------

## Stereo4D (400 video clips with 16 frames each at 5 fps): LPIPS<=0.242
| RK | Model <br />*Links:*<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;Repository&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;LPIPS&nbsp;‚Üì&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![3DV](https://img.shields.io/badge/2026-3DV-8fcaff)](https://arxiv.org/abs/2505.16565)<br />Table 1<br />M2SVid |
|:---:|:---:|:---:|
| 1 | **M2SVid**<br />[![3DV](https://img.shields.io/badge/2026-3DV-8fcaff)](https://arxiv.org/abs/2505.16565) | **0.180** {MF} |
| 2 | **SVG**<br />[![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=sx2jXZuhIx) [![GitHub Stars](https://img.shields.io/github/stars/google/Stereoscopic-Video-Generation-via-Denoising-Frame-Matrix)](https://github.com/google/Stereoscopic-Video-Generation-via-Denoising-Frame-Matrix) | **0.217** {MF} |
| 3 | **StereoCrafter**<br />[![arXiv](https://img.shields.io/badge/2024-arXiv-b31b1b)](https://arxiv.org/abs/2409.07447) [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/StereoCrafter)](https://github.com/TencentARC/StereoCrafter) | **0.242** {MF} |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## StereoWorld-11M (1000 video clips with 81 frames each at 12 fps): LPIPS<=0.1869
| RK | Model <br />*Links:*<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;Repository&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;LPIPS&nbsp;‚Üì&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2512.09363)<br />Table 2<br />StereoWorld |
|:---:|:---:|:---:|
| 1 | **StereoWorld**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2512.09363) | **0.0952** {MF} |
| 2 | **StereoCrafter**<br />[![arXiv](https://img.shields.io/badge/2024-arXiv-b31b1b)](https://arxiv.org/abs/2409.07447) [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/StereoCrafter)](https://github.com/TencentARC/StereoCrafter) | **0.1869** {MF} |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## Infinigen: OPW<=0.054
| RK | Model <br />*Links:*<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;Repository&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;OPW&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2026-arXiv-b31b1b)](https://arxiv.org/abs/2601.02793)<br />Table 2<br />StableDPT |
|:---:|:---:|:---:|
| 1 | **StableDPT**<br />[![arXiv](https://img.shields.io/badge/2026-arXiv-b31b1b)](https://arxiv.org/abs/2601.02793) | **0.023** {MF} |
| 2 | **VDA-L**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Video-Depth-Anything)](https://github.com/DepthAnything/Video-Depth-Anything) | **0.026** {MF} |
| 3 | **Depth Anything V2 Large**<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html) [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Depth-Anything-V2)](https://github.com/DepthAnything/Depth-Anything-V2) | **0.039** {1} |
| 4 | **FlashDepth**<br />[![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution_ICCV_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/Eyeline-Labs/FlashDepth)](https://github.com/Eyeline-Labs/FlashDepth) | **0.054** {MF} |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## ScanNet (170 frames): TAE<=2.2
| RK | Model <br />*Links:*<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;Repository&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;TAE&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html)<br />Table 1<br />VDA |
|:---:|:---:|:---:|
| 1 | **VDA-L**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Video-Depth-Anything)](https://github.com/DepthAnything/Video-Depth-Anything) | **0.570** {MF} |
| 2 | **DepthCrafter**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/Tencent/DepthCrafter)](https://github.com/Tencent/DepthCrafter) | **0.639** {MF} |
| 3 | **Depth Any Video**<br />[![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=gWqFbnKsqR) [![GitHub Stars](https://img.shields.io/github/stars/Nightmare-n/DepthAnyVideo)](https://github.com/Nightmare-n/DepthAnyVideo) | **0.967** {MF} |
| 4 | **ChronoDepth**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Shao_Learning_Temporally_Consistent_Video_Depth_from_Video_Diffusion_Priors_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/jhaoshao/ChronoDepth)](https://github.com/jhaoshao/ChronoDepth) | **1.022** {MF} |
| 5 | **Depth Anything V2 Large**<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html) [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Depth-Anything-V2)](https://github.com/DepthAnything/Depth-Anything-V2) | **1.140** {1} |
| 6 | **NVDS**<br />[![ICCV](https://img.shields.io/badge/2023-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Neural_Video_Depth_Stabilizer_ICCV_2023_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/RaymondWang987/NVDS)](https://github.com/RaymondWang987/NVDS) | **2.176** {4} |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## Bonn RGB-D Dynamic (5 video clips with 110 frames each): Œ¥<sub>1</sub>>=0.979
üìù **Note:** See [Figure 4](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html)
| RK | Model <br />*Links:*<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;Repository&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_SpatialTrackerV2_Advancing_3D_Point_Tracking_with_Explicit_Camera_Motion_ICCV_2025_paper.html)<br />Table 2<br />ST2 | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a_CVPR_2025_paper.html)<br />Table 2<br />Uni4D | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html)<br />Table S1<br />VDA |
|:---:|:---:|:---:|:---:|:---:|
| 1 | **SpatialTrackerV2**<br />[![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_SpatialTrackerV2_Advancing_3D_Point_Tracking_with_Explicit_Camera_Motion_ICCV_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/henry123-boy/SpaTrackerV2)](https://github.com/henry123-boy/SpaTrackerV2) | **0.988** {MF} | - | - |
| 2 | **Depth Pro**<br />[![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=aueXfY0Clv) [![GitHub Stars](https://img.shields.io/github/stars/apple/ml-depth-pro)](https://github.com/apple/ml-depth-pro) | - | **0.986** {1} | - |
| 3-4 | **Metric3D v2**<br />[![TPAMI](https://img.shields.io/badge/2024-TPAMI-fefd02)](https://ieeexplore.ieee.org/document/10638254) [![GitHub Stars](https://img.shields.io/github/stars/YvanYin/Metric3D)](https://github.com/YvanYin/Metric3D) | - | **0.985** {1} | - |
| 3-4 | **UniDepth**<br />[![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Piccinelli_UniDepth_Universal_Monocular_Metric_Depth_Estimation_CVPR_2024_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/lpiccinelli-eth/UniDepth)](https://github.com/lpiccinelli-eth/UniDepth) | - | **0.985** {1} | - |
| 5 | **Uni4D**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/Davidyao99/uni4d)](https://github.com/Davidyao99/uni4d) | - | **0.983** {MF} | - |
| 6 | **VDA-L**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Video-Depth-Anything)](https://github.com/DepthAnything/Video-Depth-Anything) | **0.982** {MF} | - | **0.972** {MF} |
| 7 | **Depth Any Video**<br />[![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=gWqFbnKsqR) [![GitHub Stars](https://img.shields.io/github/stars/Nightmare-n/DepthAnyVideo)](https://github.com/Nightmare-n/DepthAnyVideo) | - | - | **0.981** {MF} |
| 8 | **DepthCrafter**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/Tencent/DepthCrafter)](https://github.com/Tencent/DepthCrafter) | **0.979** {MF} | **0.976** {MF} | **0.979** {MF} |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## Bonn RGB-D Dynamic (5 video clips with 110 frames each): AbsRel<=0.052
üìù **Note:** See [Figure 4](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html)
| RK | Model <br />*Links:*<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;Repository&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_SpatialTrackerV2_Advancing_3D_Point_Tracking_with_Explicit_Camera_Motion_ICCV_2025_paper.html)<br />Table 2<br />ST2 | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a_CVPR_2025_paper.html)<br />Table 2<br />Uni4D | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2510.27234)<br />Table 7<br />MoRE | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2507.13347)<br />Table 5<br />œÄ<sup>3</sup> | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html)<br />Table S1<br />VDA |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 1 | **SpatialTrackerV2**<br />[![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://arxiv.org/abs/2507.12462) [![GitHub Stars](https://img.shields.io/github/stars/henry123-boy/SpaTrackerV2)](https://github.com/henry123-boy/SpaTrackerV2) | **0.028** {MF} | - | - | - | - |
| 2 | **MegaSaM**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/mega-sam/mega-sam)](https://github.com/mega-sam/mega-sam) | **0.037** {MF} | - | - | - | - |
| 3 | **Uni4D**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/Davidyao99/uni4d)](https://github.com/Davidyao99/uni4d) | - | **0.038** {MF} | - | - | - |
| 4 | **UniDepth**<br />[![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Piccinelli_UniDepth_Universal_Monocular_Metric_Depth_Estimation_CVPR_2024_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/lpiccinelli-eth/UniDepth)](https://github.com/lpiccinelli-eth/UniDepth) | - | **0.040** {1} | - | - | - |
| 5 | **MoRE**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2510.27234) [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Taobao3D)](https://github.com/alibaba/Taobao3D) | - | - | **0.042** {MF} | - | - |
| 6 | **œÄ<sup>3</sup>**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2507.13347) [![GitHub Stars](https://img.shields.io/github/stars/yyfz/Pi3)](https://github.com/yyfz/Pi3) | - | - | **0.043** {MF} | **0.043** {MF} | - |
| 7 | **Metric3D v2**<br />[![TPAMI](https://img.shields.io/badge/2024-TPAMI-fefd02)](https://ieeexplore.ieee.org/document/10638254) [![GitHub Stars](https://img.shields.io/github/stars/YvanYin/Metric3D)](https://github.com/YvanYin/Metric3D) | - | **0.044** {1} | - | - | - |
| 8-9 | **Depth Pro**<br />[![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=aueXfY0Clv) [![GitHub Stars](https://img.shields.io/github/stars/apple/ml-depth-pro)](https://github.com/apple/ml-depth-pro) | - | **0.049** {1} | - | - | - |
| 8-9 | **VDA-L**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Video-Depth-Anything)](https://github.com/DepthAnything/Video-Depth-Anything) | **0.049** {MF} | - | - | - | **0.053** {MF} |
| 10 | **Depth Any Video**<br />[![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=gWqFbnKsqR) [![GitHub Stars](https://img.shields.io/github/stars/Nightmare-n/DepthAnyVideo)](https://github.com/Nightmare-n/DepthAnyVideo) | - | - | - | - | **0.051** {MF} |
| 11 | **VGGT**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/vggt)](https://github.com/facebookresearch/vggt) | **0.056** {MF} | - | **0.052** {MF} | **0.052** {MF} | - |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## DIODE: Œ¥<sub>1</sub>>=0.953
| RK | Model <br />*Links:*<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;Repository&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2511.10647)<br />Table 4<br />DA3 | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Xu_GeometryCrafter_Consistent_Geometry_Estimation_for_Open-world_Videos_with_Diffusion_Priors_ICCV_2025_paper.html)<br />Table 2<br />GC | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![NeurIPS](https://img.shields.io/badge/2025-NeurIPS-68448a)](https://arxiv.org/abs/2510.07316)<br />Table 1<br />PPD | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html)<br />Table 2<br />DA V2 |
|:---:|:---:|:---:|:---:|:---:|:---:|
| 1 | **DA3 Teacher**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2511.10647) [![GitHub Stars](https://img.shields.io/github/stars/ByteDance-Seed/Depth-Anything-3)](https://github.com/ByteDance-Seed/Depth-Anything-3) | **0.966** {1} | - | - | - |
| 2 | **GeometryCrafter(D)**<br />[![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Xu_GeometryCrafter_Consistent_Geometry_Estimation_for_Open-world_Videos_with_Diffusion_Priors_ICCV_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/GeometryCrafter)](https://github.com/TencentARC/GeometryCrafter) | - | **0.962** {1} | - | - |
| 3 | **Pixel-Perfect Depth 1024**<br />[![NeurIPS](https://img.shields.io/badge/2025-NeurIPS-68448a)](https://arxiv.org/abs/2510.07316) [![GitHub Stars](https://img.shields.io/github/stars/gangweix/pixel-perfect-depth)](https://github.com/gangweix/pixel-perfect-depth) | - | - | **0.959** {1} | - |
| 4 | **Depth Anything V2 Giant**<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html) [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Depth-Anything-V2)](https://github.com/DepthAnything/Depth-Anything-V2) | - | - | - | **0.954** {1} |
| 5 | **VGGT**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/vggt)](https://github.com/facebookresearch/vggt) | **0.953** {1} | - | - | - |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## NYU-Depth V2: Œ¥<sub>1</sub>>=0.983
| RK | Model <br />*Links:*<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;Repository&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.html)<br />Table A2<br />MoGe | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2507.02546)<br />Table B.4<br />MoGe-2 | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2502.19204)<br />Table 5<br />DAD | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2505.23400)<br />Table 1<br />BriGeS | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Depth_Anything_Unleashing_the_Power_of_Large-Scale_Unlabeled_Data_CVPR_2024_paper.html)<br />Table 2<br />DA | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Œ¥<sub>1</sub>&nbsp;‚Üë&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html)<br />Table 2<br />DA V2 |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 1-2 | **UniDepth**<br />[![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Piccinelli_UniDepth_Universal_Monocular_Metric_Depth_Estimation_CVPR_2024_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/lpiccinelli-eth/UniDepth)](https://github.com/lpiccinelli-eth/UniDepth) | **0,987** {1} | **0.987** {1} | - | - | - | - |
| 1-2 | **UniDepthV2**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2502.20110) [![GitHub Stars](https://img.shields.io/github/stars/lpiccinelli-eth/UniDepth)](https://github.com/lpiccinelli-eth/UniDepth) | - | **0.987** {1} | - | - | - | - |
| 3-4 | **MoGe**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/microsoft/MoGe)](https://github.com/microsoft/MoGe) | **0,986** {1} | **0.986** {1} | - | - | - | - |
| 3-4 | **MoGe-2**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2507.02546) [![GitHub Stars](https://img.shields.io/github/stars/microsoft/MoGe)](https://github.com/microsoft/MoGe) | - | **0.986** {1} | - | - | - | - |
| 5 | **Distill Any Depth<sup>‚Ä†</sup>**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2502.19204) [![GitHub Stars](https://img.shields.io/github/stars/Westlake-AGI-Lab/Distill-Any-Depth)](https://github.com/Westlake-AGI-Lab/Distill-Any-Depth) | - | - | **0.985** {1} | - | - | - |
| 6-7 | **DA Large + BriGeS**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2505.23400) | - | - | - | **0.984** {1} | - | - |
| 6-7 | **Depth Anything Large**<br />[![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Depth_Anything_Unleashing_the_Power_of_Large-Scale_Unlabeled_Data_CVPR_2024_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/LiheYoung/Depth-Anything)](https://github.com/LiheYoung/Depth-Anything) | **0,984** {1} | **0.984** {1} | - | **0.981** {1} | **0.981** {1} | **0.981** {1} |
| 8 | **Depth Anything V2 Large**<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html) [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Depth-Anything-V2)](https://github.com/DepthAnything/Depth-Anything-V2) | **0,983** {1} | **0.983** {1} | **0.979** {1} | **0.979** {1} | - | **0.979** {1} |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## NYU-Depth V2: AbsRel<=0.0421
| RK | Model <br />*Links:*<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;Repository&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2507.02546)<br />Table B.4<br />MoGe-2 | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.html)<br />Table A2<br />MoGe | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2505.23400)<br />Table 1<br />BriGeS | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2509.25077)<br />Table 1<br />BRIDGE | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2509.04338)<br />Table 2<br />FE2E | &nbsp;&nbsp;&nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![NeurIPS](https://img.shields.io/badge/2025-NeurIPS-68448a)](https://arxiv.org/abs/2510.07316)<br />Table 1<br />PPD | &nbsp;&nbsp;&nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html)<br />Table 2<br />DA V2 | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/c4b652b7e228b18e1c65478da3a4a2cf-Abstract-Conference.html)<br />Table 2<br />BD | &nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Depth_Anything_Unleashing_the_Power_of_Large-Scale_Unlabeled_Data_CVPR_2024_paper.html)<br />Table 2<br />DA | &nbsp;&nbsp;&nbsp;AbsRel&nbsp;‚Üì&nbsp;&nbsp;&nbsp;<br />{Input&nbsp;fr.}<br />[![arXiv](https://img.shields.io/badge/2024-arXiv-b31b1b)](https://arxiv.org/abs/2404.15506)<br />Table 4<br />M3D v2 |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 1 | **MoGe-2**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2507.02546) [![GitHub Stars](https://img.shields.io/github/stars/microsoft/MoGe)](https://github.com/microsoft/MoGe) | **0.0335** {1} | - | - | - | - | - | - | - | - | - |
| 2-3 | **MoGe**<br />[![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/microsoft/MoGe)](https://github.com/microsoft/MoGe) | **0.0338** {1} | **0.0338** {1} | - | - | - | - | - | - | - | - |
| 2-3 | **UniDepthV2**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2502.20110) [![GitHub Stars](https://img.shields.io/github/stars/lpiccinelli-eth/UniDepth)](https://github.com/lpiccinelli-eth/UniDepth) | **0.0338** {1} | - | - | - | - | - | - | - | - | - |
| 4 | **UniDepth**<br />[![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Piccinelli_UniDepth_Universal_Monocular_Metric_Depth_Estimation_CVPR_2024_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/lpiccinelli-eth/UniDepth)](https://github.com/lpiccinelli-eth/UniDepth) | **0.0378** {1} | **0.0378** {1} | - | - | - | - | - | - | - | - |
| 5 | **DA Large + BriGeS**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2505.23400) | - | - | **0.040** {1} | - | - | - | - | - | - | - |
| 6-8 | **BRIDGE**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2509.25077) [![GitHub Stars](https://img.shields.io/github/stars/lnbxldn/Bridge)](https://github.com/lnbxldn/Bridge) | - | - | - | **0.041** {1} | - | - | - | - | - | - |
| 6-8 | **FE2E**<br />[![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2509.04338) [![GitHub Stars](https://img.shields.io/github/stars/AMAP-ML/FE2E)](https://github.com/AMAP-ML/FE2E) | - | - | - | - | **0.041** {1} | - | - | - | - | - |
| 6-8 | **Pixel-Perfect Depth 1024**<br />[![NeurIPS](https://img.shields.io/badge/2025-NeurIPS-68448a)](https://arxiv.org/abs/2510.07316) [![GitHub Stars](https://img.shields.io/github/stars/gangweix/pixel-perfect-depth)](https://github.com/gangweix/pixel-perfect-depth) | - | - | - | - | - | **0.041** {1} | - | - | - | - |
| 9 | **Depth Anything V2 Large**<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html) [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Depth-Anything-V2)](https://github.com/DepthAnything/Depth-Anything-V2) | **0.0414** {1} | **0.0414** {1} | **0.043** {1} | **0.045** {1} | **0.045** {1} | **0.045** {1} | **0.045** {1} | - | - | - |
| 10-12 | **BetterDepth**<br />[![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/c4b652b7e228b18e1c65478da3a4a2cf-Abstract-Conference.html) | - | - | - | - | - | - | - | **0.042** {1} | - | - |
| 10-12 | **Depth Anything Large**<br />[![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Depth_Anything_Unleashing_the_Power_of_Large-Scale_Unlabeled_Data_CVPR_2024_paper.html) [![GitHub Stars](https://img.shields.io/github/stars/LiheYoung/Depth-Anything)](https://github.com/LiheYoung/Depth-Anything) | **0.0420** {1} | **0.0420** {1} | **0.042** {1} | - | **0.043** {1} | - | **0.043** {1} | **0.043** {1} | **0.043** {1} | **0.043** {1} |
| 10-12 | **Metric3D v2 ViT-Large**<br />[![TPAMI](https://img.shields.io/badge/2024-TPAMI-fefd02)](https://ieeexplore.ieee.org/document/10638254) [![GitHub Stars](https://img.shields.io/github/stars/YvanYin/Metric3D)](https://github.com/YvanYin/Metric3D) | **0.134** {1} | **0.134** {1} | - | **0.058** {1} | - | - | - | - | - | **0.042** {1} |
| 13 | **Depth Pro**<br />[![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=aueXfY0Clv) [![GitHub Stars](https://img.shields.io/github/stars/apple/ml-depth-pro)](https://github.com/apple/ml-depth-pro) | **0.0421** {1} | - | - | **0.245** {1} | - | - | - | - | - | - |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## Appendix 4: Notes for "Awesome Synthetic RGB-D Video Datasets for Training and Testing HD Video Depth Estimation Models"
üìù **Note 1:** Example of arranging images in the correct order to make a 32-frame video sequence for the [ClaraVid](https://rdbch.github.io/claravid/) dataset:
```
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00360.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00320.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00280.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00240.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00200.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00160.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00120.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00080.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00040.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00000.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00001.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00002.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00003.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00004.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00005.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00006.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00007.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00008.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00009.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00010.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00011.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00012.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00013.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00014.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00015.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00016.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00017.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00018.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00019.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00059.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00099.jpg
<your-data-path>/008_urban_dense_1/left_rgb/45deg_low_h/00139.jpg
```
üìù **Note 2:** Do not use the [SYNTHIA-Seqs](https://synthia-dataset.net/) dataset for training HD video depth estimation models! The depth maps in this dataset do not match the corresponding RGB images. This is particularly evident in the example of tree leaves:  
`<your-data-path>/SYNTHIA-SEQS-01-SPRING/Depth/Stereo_Left/Omni_F/000071.png`  
`<your-data-path>/SYNTHIA-SEQS-01-SPRING/RGB/Stereo_Left/Omni_F/000071.png`.  
üìù **Note 3:** Do not use the [DigiDogs](https://cvssp.org/data/DigiDogs/) dataset for training HD video depth estimation models! The depth maps in this dataset do not match the corresponding RGB images. See the objects behind the campfire, the shifting position of the vegetation on the left and the clear banding on the depth map:  
`<your-data-path>/DigiDogs2024_full/09_22_2022/00054/images/img_00012.tiff`.  
üìù **Note 4:** Check before use the [SynDrone](https://github.com/LTTM/Syndrone) dataset for training HD video depth estimation models! The depth maps in this dataset have large white areas of unknown depth, which should not happen with a synthetic dataset. Example depth map:  
`<your-data-path>/Town01_Opt_120_depth/Town01_Opt_120/ClearNoon/height20m/depth/00031.png`.  
üìù **Note 5:** Check before use the [Aria Synthetic Environments](https://www.projectaria.com/datasets/ase/) dataset for training HD video depth estimation models! The depth maps in this dataset have large white areas of unknown depth, which should not happen with a synthetic dataset. Example depth map:  
`<your-data-path>/75/depth/depth0000109.png`.

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## Appendix 5: List of all research papers from the above rankings
| Method | Abbr. | Paper | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />(Alt link) | Official<br />&nbsp;&nbsp;repository&nbsp;&nbsp; |
|:---:|:---:|:---:|:---:|:---:|
| BetterDepth | BD | BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation | [![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/c4b652b7e228b18e1c65478da3a4a2cf-Abstract-Conference.html) | - |
| BRIDGE | - | BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2509.25077) | [![GitHub Stars](https://img.shields.io/github/stars/lnbxldn/Bridge)](https://github.com/lnbxldn/Bridge) |
| BriGeS | - | Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2505.23400) | - |
| ChronoDepth | - | Learning Temporally Consistent Video Depth from Video Diffusion Priors | [![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Shao_Learning_Temporally_Consistent_Video_Depth_from_Video_Diffusion_Priors_CVPR_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/jhaoshao/ChronoDepth)](https://github.com/jhaoshao/ChronoDepth) |
| Depth Any Video | DAV | Depth Any Video with Scalable Synthetic Data | [![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=gWqFbnKsqR) | [![GitHub Stars](https://img.shields.io/github/stars/Nightmare-n/DepthAnyVideo)](https://github.com/Nightmare-n/DepthAnyVideo) |
| Depth Anything | DA | Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data | [![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Depth_Anything_Unleashing_the_Power_of_Large-Scale_Unlabeled_Data_CVPR_2024_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/LiheYoung/Depth-Anything)](https://github.com/LiheYoung/Depth-Anything) |
| Depth Anything 3 | DA3 | Depth Anything 3: Recovering the Visual Space from Any Views | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2511.10647) | [![GitHub Stars](https://img.shields.io/github/stars/ByteDance-Seed/Depth-Anything-3)](https://github.com/ByteDance-Seed/Depth-Anything-3) |
| Depth Anything V2 | DA V2 | Depth Anything V2 | [![NeurIPS](https://img.shields.io/badge/2024-NeurIPS-68448a)](https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html) | [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Depth-Anything-V2)](https://github.com/DepthAnything/Depth-Anything-V2) |
| Depth Pro | DP | Depth Pro: Sharp Monocular Metric Depth in Less Than a Second | [![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=aueXfY0Clv) | [![GitHub Stars](https://img.shields.io/github/stars/apple/ml-depth-pro)](https://github.com/apple/ml-depth-pro) |
| DepthCrafter | DC | DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos | [![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos_CVPR_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/Tencent/DepthCrafter)](https://github.com/Tencent/DepthCrafter) |
| Distill Any Depth | DAD | Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2502.19204) | [![GitHub Stars](https://img.shields.io/github/stars/Westlake-AGI-Lab/Distill-Any-Depth)](https://github.com/Westlake-AGI-Lab/Distill-Any-Depth) |
| FE2E | - | From Editor to Dense Geometry Estimator | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2509.04338) | [![GitHub Stars](https://img.shields.io/github/stars/AMAP-ML/FE2E)](https://github.com/AMAP-ML/FE2E) |
| FlashDepth | - | FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution | [![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution_ICCV_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/Eyeline-Labs/FlashDepth)](https://github.com/Eyeline-Labs/FlashDepth) |
| GeometryCrafter | GC | GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors | [![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Xu_GeometryCrafter_Consistent_Geometry_Estimation_for_Open-world_Videos_with_Diffusion_Priors_ICCV_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/GeometryCrafter)](https://github.com/TencentARC/GeometryCrafter) |
| M2SVid | - | M2SVid: End-to-End Inpainting and Refinement for Monocular-to-Stereo Video Conversion | [![3DV](https://img.shields.io/badge/2026-3DV-8fcaff)](https://arxiv.org/abs/2505.16565) | - |
| MegaSaM | - | MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos | [![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/mega-sam/mega-sam)](https://github.com/mega-sam/mega-sam) |
| Metric3D v2 | M3D v2 | Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation | [![TPAMI](https://img.shields.io/badge/2024-TPAMI-fefd02)](https://ieeexplore.ieee.org/document/10638254)<br />[![arXiv](https://img.shields.io/badge/2024-arXiv-b31b1b)](https://arxiv.org/abs/2404.15506) | [![GitHub Stars](https://img.shields.io/github/stars/YvanYin/Metric3D)](https://github.com/YvanYin/Metric3D) |
| MoGe | - | MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision | [![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/MoGe)](https://github.com/microsoft/MoGe) |
| MoGe-2 | Mo2 | MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2507.02546) | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/MoGe)](https://github.com/microsoft/MoGe) |
| MoRE | - | MoRE: 3D Visual Geometry Reconstruction Meets Mixture-of-Experts | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2510.27234) | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Taobao3D)](https://github.com/alibaba/Taobao3D) |
| NVDS | - | Neural Video Depth Stabilizer | [![ICCV](https://img.shields.io/badge/2023-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Neural_Video_Depth_Stabilizer_ICCV_2023_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/RaymondWang987/NVDS)](https://github.com/RaymondWang987/NVDS) |
| Pixel-Perfect Depth | PPD | Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers | [![NeurIPS](https://img.shields.io/badge/2025-NeurIPS-68448a)](https://arxiv.org/abs/2510.07316) | [![GitHub Stars](https://img.shields.io/github/stars/gangweix/pixel-perfect-depth)](https://github.com/gangweix/pixel-perfect-depth) |
| SpatialTrackerV2 | ST2 | SpatialTrackerV2: 3D Point Tracking Made Easy | [![ICCV](https://img.shields.io/badge/2025-ICCV-fcb900)](https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_SpatialTrackerV2_Advancing_3D_Point_Tracking_with_Explicit_Camera_Motion_ICCV_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/henry123-boy/SpaTrackerV2)](https://github.com/henry123-boy/SpaTrackerV2) |
| StableDPT | - | StableDPT: Temporal Stable Monocular Video Depth Estimation | [![arXiv](https://img.shields.io/badge/2026-arXiv-b31b1b)](https://arxiv.org/abs/2601.02793) | - |
| StereoCrafter | - | StereoCrafter: Diffusion-based Generation of Long and High-fidelity Stereoscopic 3D from Monocular Videos | [![arXiv](https://img.shields.io/badge/2024-arXiv-b31b1b)](https://arxiv.org/abs/2409.07447) | [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/StereoCrafter)](https://github.com/TencentARC/StereoCrafter) |
| StereoWorld | - | StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2512.09363) | - |
| SVG | - | SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix | [![ICLR](https://img.shields.io/badge/2025-ICLR-d5df32)](https://openreview.net/forum?id=sx2jXZuhIx) | [![GitHub Stars](https://img.shields.io/github/stars/google/Stereoscopic-Video-Generation-via-Denoising-Frame-Matrix)](https://github.com/google/Stereoscopic-Video-Generation-via-Denoising-Frame-Matrix) |
| Uni4D | - | Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video | [![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a_CVPR_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/Davidyao99/uni4d)](https://github.com/Davidyao99/uni4d) |
| UniDepth | - | UniDepth: Universal Monocular Metric Depth Estimation | [![CVPR](https://img.shields.io/badge/2024-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2024/html/Piccinelli_UniDepth_Universal_Monocular_Metric_Depth_Estimation_CVPR_2024_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/lpiccinelli-eth/UniDepth)](https://github.com/lpiccinelli-eth/UniDepth) |
| UniDepthV2 | UD2 | UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2502.20110) | [![GitHub Stars](https://img.shields.io/github/stars/lpiccinelli-eth/UniDepth)](https://github.com/lpiccinelli-eth/UniDepth) |
| VGGT | - | VGGT: Visual Geometry Grounded Transformer | [![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/vggt)](https://github.com/facebookresearch/vggt) |
| Video Depth Anything | VDA | Video Depth Anything: Consistent Depth Estimation for Super-Long Videos | [![CVPR](https://img.shields.io/badge/2025-CVPR-1e407f)](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html) | [![GitHub Stars](https://img.shields.io/github/stars/DepthAnything/Video-Depth-Anything)](https://github.com/DepthAnything/Video-Depth-Anything) |
| œÄ<sup>3</sup> | - | œÄ<sup>3</sup>: Scalable Permutation-Equivariant Visual Geometry Learning | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2507.13347) | [![GitHub Stars](https://img.shields.io/github/stars/yyfz/Pi3)](https://github.com/yyfz/Pi3) |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)

## List of research papers to be added to the rankings
| Method | Abbr. | Paper | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Venue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />(Alt link) | Official<br />&nbsp;&nbsp;repository&nbsp;&nbsp; |
|:---:|:---:|:---:|:---:|:---:|
| HairGuard | - | Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views | [![arXiv](https://img.shields.io/badge/2026-arXiv-b31b1b)](https://arxiv.org/abs/2601.03362) | - |
| StereoPilot | - | StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2512.16915) | [![GitHub Stars](https://img.shields.io/github/stars/KlingTeam/StereoPilot)](https://github.com/KlingTeam/StereoPilot) |
| Elastic3D | - | Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2512.14236) | - |
| Restereo | - | Restereo: Diffusion stereo video generation and restoration | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2506.06023) | - |
| Eye2Eye | - | Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis | [![arXiv](https://img.shields.io/badge/2025-arXiv-b31b1b)](https://arxiv.org/abs/2505.00135) | - |

[![Back to Top](https://img.shields.io/badge/Back_to_Top-555555)](#video-depth-estimation-rankingsand-stereo-video-conversion-rankings)
[![Back to the List of Rankings](https://img.shields.io/badge/Back_to_the_List_of_Rankings-555555)](#list-of-rankings)
